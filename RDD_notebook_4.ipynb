{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "Welcome to exercise one of week two of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise you\u2019ll read a DataFrame in order to perform a simple statistical analysis. Then you\u2019ll rebalance the dataset. No worries, we\u2019ll explain everything to you, let\u2019s get started.\n\nLet\u2019s create a data frame from a remote file by downloading it:\n\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200128072852-0000\nKERNEL_ID = f1b039e4-9285-4877-be94-c43b0d09670c\n--2020-01-28 07:28:57--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.118.4\nConnecting to github.com (github.com)|140.82.118.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet [following]\n--2020-01-28 07:28:57--  https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.60.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.60.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.04s   \n\n2020-01-28 07:28:58 (23.3 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n"
                }
            ],
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let\u2019s have a look at the data set first. This dataset contains sensor recordings from different movement activities as we will see in the next week\u2019s lectures. X, Y and Z contain accelerometer sensor values whereas the class field contains information about which movement has been recorded. The source field is optional and can be used for data lineage since it contains the file name of the original file where the particular row was imported from.\n\nMore details on the data set can be found here:\nhttps://github.com/wchill/HMP_Dataset\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\nroot\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n\n"
                }
            ],
            "source": "df.show()\ndf.printSchema()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This is a classical classification data set. One thing we always do during data analysis is checking if the classes are balanced. In other words, if there are more or less the same number of example in each class. Let\u2019s find out by a simple aggregation using SQL."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------+--------+\n|         class|count(1)|\n+--------------+--------+\n| Use_telephone|   15225|\n| Standup_chair|   25417|\n|      Eat_meat|   31236|\n|     Getup_bed|   45801|\n|   Drink_glass|   42792|\n|    Pour_water|   41673|\n|     Comb_hair|   23504|\n|          Walk|   92254|\n|  Climb_stairs|   40258|\n| Sitdown_chair|   25036|\n|   Liedown_bed|   11446|\n|Descend_stairs|   15375|\n|   Brush_teeth|   29829|\n|      Eat_soup|    6683|\n+--------------+--------+\n\n"
                }
            ],
            "source": "spark.sql('select class,count(*) from df group by class').show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As you can see there is quite an imbalance between classes. Before we dig into this, let\u2019s re-write the same query using the DataFrame API \u2013 just in case you are not familiar with SQL. As we\u2019ve learned before, it doesn\u2019t matter if you express your queries with SQL or the DataFrame API \u2013 it all gets boiled down into the same execution plan optimized by Tungsten and accelerated by Catalyst. You can even mix and match SQL and DataFrame API code if you like.\n\nAgain, more details on the API can be found here:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n| Use_telephone|15225|\n| Standup_chair|25417|\n|      Eat_meat|31236|\n|     Getup_bed|45801|\n|   Drink_glass|42792|\n|    Pour_water|41673|\n|     Comb_hair|23504|\n|          Walk|92254|\n|  Climb_stairs|40258|\n| Sitdown_chair|25036|\n|   Liedown_bed|11446|\n|Descend_stairs|15375|\n|   Brush_teeth|29829|\n|      Eat_soup| 6683|\n+--------------+-----+\n\n"
                }
            ],
            "source": "df.groupBy('class').count().show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let\u2019s create a bar plot from this data. We\u2019re using the pixidust library, which is Open Source, because of its simplicity. But any other library like matplotlib is fine as well. "
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "barChart",
                        "keyFields": "class",
                        "legend": "true",
                        "mpld3": "false",
                        "orientation": "horizontal",
                        "rendererId": "matplotlib",
                        "sortby": "Values ASC",
                        "valueFields": "count"
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/html": "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>",
                        "text/plain": "<IPython.core.display.HTML object>"
                    },
                    "metadata": {
                        "pixieapp_metadata": null
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "import pixiedust\nfrom pyspark.sql.functions import col\ncounts = df.groupBy('class').count().orderBy('count')\ndisplay(counts)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This looks nice, but it would be nice if we can aggregate further to obtain some quantitative metrics on the imbalance like, min, max, mean and standard deviation. If we divide max by min we get a measure called minmax ration which tells us something about the relationship between the smallest and largest class. Again, let\u2019s first use SQL for those of you familiar with SQL. Don\u2019t be scared, we\u2019re used nested sub-selects, basically selecting from a result of a SQL query like it was a table. All within on SQL statement."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+----+-----+------------------+------------------+-----------------+\n| min|  max|              mean|            stddev|      minmaxratio|\n+----+-----+------------------+------------------+-----------------+\n|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n+----+-----+------------------+------------------+-----------------+\n\n"
                }
            ],
            "source": "spark.sql('''\n    select \n        *,\n        max/min as minmaxratio -- compute minmaxratio based on previously computed values\n        from (\n            select \n                min(ct) as min, -- compute minimum value of all classes\n                max(ct) as max, -- compute maximum value of all classes\n                mean(ct) as mean, -- compute mean between all classes\n                stddev(ct) as stddev -- compute standard deviation between all classes\n                from (\n                    select\n                        count(*) as ct -- count the number of rows per class and rename it to ct\n                        from df -- access the temporary query table called df backed by DataFrame df\n                        group by class -- aggrecate over class\n                )\n        )   \n''').show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The same query can be expressed using the DataFrame API. Again, don\u2019t be scared. It\u2019s just a sequential expression of transformation steps. You now an choose which syntax you like better."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+----+-----+------------------+------------------+-----------------+\n| min|  max|              mean|            stddev|      minmaxratio|\n+----+-----+------------------+------------------+-----------------+\n|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n+----+-----+------------------+------------------+-----------------+\n\n"
                }
            ],
            "source": "from pyspark.sql.functions import col, min, max, mean, stddev\n\ndf \\\n    .groupBy('class') \\\n    .count() \\\n    .select([ \n        min(col(\"count\")).alias('min'), \n        max(col(\"count\")).alias('max'), \n        mean(col(\"count\")).alias('mean'), \n        stddev(col(\"count\")).alias('stddev') \n    ]) \\\n    .select([\n        col('*'),\n        (col(\"max\") / col(\"min\")).alias('minmaxratio')\n    ]) \\\n    .show()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now it\u2019s time for you to work on the data set. First, please create a table of all classes with the respective counts, but this time, please order the table by the count number, ascending."
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n|      Eat_soup| 6683|\n|   Liedown_bed|11446|\n| Use_telephone|15225|\n|Descend_stairs|15375|\n|     Comb_hair|23504|\n| Sitdown_chair|25036|\n| Standup_chair|25417|\n|   Brush_teeth|29829|\n|      Eat_meat|31236|\n|  Climb_stairs|40258|\n|    Pour_water|41673|\n|   Drink_glass|42792|\n|     Getup_bed|45801|\n|          Walk|92254|\n+--------------+-----+\n\n"
                }
            ],
            "source": "rdd=df.groupBy('class').count().sort('count')\nrdd.show()\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Pixiedust is a very sophisticated library. It takes care of sorting as well. Please modify the bar chart so that it gets sorted by the number of elements per class, ascending. Hint: It\u2019s an option available in the UI once rendered using the display() function."
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "tableView"
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/html": "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>\n        <div class=\"pd_save is-viewer-good\" style=\"padding-right:10px;text-align: center;line-height:initial !important;font-size: xx-large;font-weight: 500;color: coral;\">\n            \n        </div>\n    <div id=\"chartFigured3e86100\" class=\"pd_save is-viewer-good\" style=\"overflow-x:auto\">\n            <style type=\"text/css\" class=\"pd_save\">\n    .df-table-wrapper .panel-heading {\n      border-radius: 0;\n      padding: 0px;\n    }\n    .df-table-wrapper .panel-heading:hover {\n      border-color: #008571;\n    }\n    .df-table-wrapper .panel-title a {\n      background-color: #f9f9fb;\n      color: #333333;\n      display: block;\n      outline: none;\n      padding: 10px 15px;\n      text-decoration: none;\n    }\n    .df-table-wrapper .panel-title a:hover {\n      background-color: #337ab7;\n      border-color: #2e6da4;\n      color: #ffffff;\n      display: block;\n      padding: 10px 15px;\n      text-decoration: none;\n    }\n    .df-table-wrapper {\n      font-size: small;\n      font-weight: 300;\n      letter-spacing: 0.5px;\n      line-height: normal;\n      height: inherit;\n      overflow: auto;\n    }\n    .df-table-search {\n      margin: 0 0 20px 0;\n    }\n    .df-table-search-count {\n      display: inline-block;\n      margin: 0 0 20px 0;\n    }\n    .df-table-container {\n      max-height: 50vh;\n      max-width: 100%;\n      overflow-x: auto;\n      position: relative;\n    }\n    .df-table-wrapper table {\n      border: 0 none #ffffff;\n      border-collapse: collapse;\n      margin: 0;\n      min-width: 100%;\n      padding: 0;\n      table-layout: fixed;\n      height: inherit;\n      overflow: auto;\n    }\n    .df-table-wrapper tr.hidden {\n      display: none;\n    }\n    .df-table-wrapper tr:nth-child(even) {\n      background-color: #f9f9fb;\n    }\n    .df-table-wrapper tr.even {\n      background-color: #f9f9fb;\n    }\n    .df-table-wrapper tr.odd {\n      background-color: #ffffff;\n    }\n    .df-table-wrapper td + td {\n      border-left: 1px solid #e0e0e0;\n    }\n  \n    .df-table-wrapper thead,\n    .fixed-header {\n      font-weight: 600;\n    }\n    .df-table-wrapper tr,\n    .fixed-row {\n      border: 0 none #ffffff;\n      margin: 0;\n      padding: 0;\n    }\n    .df-table-wrapper th,\n    .df-table-wrapper td,\n    .fixed-cell {\n      border: 0 none #ffffff;\n      margin: 0;\n      min-width: 50px;\n      padding: 5px 20px 5px 10px;\n      text-align: left;\n      word-wrap: break-word;\n    }\n    .df-table-wrapper th {\n      padding-bottom: 0;\n      padding-top: 0;\n    }\n    .df-table-wrapper th div {\n      max-height: 1px;\n      visibility: hidden;\n    }\n  \n    .df-schema-field {\n      margin-left: 10px;\n    }\n  \n    .fixed-header-container {\n      overflow: hidden;\n      position: relative;\n    }\n    .fixed-header {\n      border-bottom: 2px solid #000;\n      display: table;\n      position: relative;\n    }\n    .fixed-row {\n      display: table-row;\n    }\n    .fixed-cell {\n      display: table-cell;\n    }\n  </style>\n  \n  \n  <div class=\"df-table-wrapper df-table-wrapper-d3e86100 panel-group pd_save\">\n    <!-- dataframe schema -->\n    \n    <div class=\"panel panel-default\">\n      <div class=\"panel-heading\">\n        <h4 class=\"panel-title\" style=\"margin: 0px;\">\n          <a data-toggle=\"collapse\" href=\"#df-schema-d3e86100\" data-parent=\"#df-table-wrapper-d3e86100\">Schema</a>\n        </h4>\n      </div>\n      <div id=\"df-schema-d3e86100\" class=\"panel-collapse collapse\">\n        <div class=\"panel-body\" style=\"font-family: monospace;\">\n          <div class=\"df-schema-fields\">\n            <div>Field types:</div>\n            \n              <div class=\"df-schema-field\"><strong>class: </strong> object</div>\n            \n              <div class=\"df-schema-field\"><strong>count: </strong> int64</div>\n            \n          </div>\n        </div>\n      </div>\n    </div>\n    \n    <!-- dataframe table -->\n    <div class=\"panel panel-default\">\n      \n      <div class=\"panel-heading\">\n        <h4 class=\"panel-title\" style=\"margin: 0px;\">\n          <a data-toggle=\"collapse\" href=\"#df-table-d3e86100\" data-parent=\"#df-table-wrapper-d3e86100\"> Table</a>\n        </h4>\n      </div>\n      \n      <div id=\"df-table-d3e86100\" class=\"panel-collapse collapse in\">\n        <div class=\"panel-body\">\n          \n          <input type=\"text\" class=\"df-table-search form-control input-sm\" placeholder=\"Search table\">\n          \n          <div>\n            \n            <span class=\"df-table-search-count\">Showing 14 of 14 rows</span>\n            \n          </div>\n          <!-- fixed header for when dataframe table scrolls -->\n          <div class=\"fixed-header-container\">\n            <div class=\"fixed-header\">\n              <div class=\"fixed-row\">\n                \n                <div class=\"fixed-cell\">class</div>\n                \n                <div class=\"fixed-cell\">count</div>\n                \n              </div>\n            </div>\n          </div>\n          <div class=\"df-table-container\">\n            <table class=\"df-table\">\n              <thead>\n                <tr>\n                  \n                  <th><div>class</div></th>\n                  \n                  <th><div>count</div></th>\n                  \n                </tr>\n              </thead>\n              <tbody>\n                \n                <tr>\n                  \n                  <td>Eat_soup</td>\n                  \n                  <td>6683</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Liedown_bed</td>\n                  \n                  <td>11446</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Use_telephone</td>\n                  \n                  <td>15225</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Descend_stairs</td>\n                  \n                  <td>15375</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Comb_hair</td>\n                  \n                  <td>23504</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Sitdown_chair</td>\n                  \n                  <td>25036</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Standup_chair</td>\n                  \n                  <td>25417</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Brush_teeth</td>\n                  \n                  <td>29829</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Eat_meat</td>\n                  \n                  <td>31236</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Climb_stairs</td>\n                  \n                  <td>40258</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Pour_water</td>\n                  \n                  <td>41673</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Drink_glass</td>\n                  \n                  <td>42792</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Getup_bed</td>\n                  \n                  <td>45801</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>Walk</td>\n                  \n                  <td>92254</td>\n                  \n                </tr>\n                \n              </tbody>\n            </table>\n          </div>\n        </div>\n      </div>\n    </div>\n  </div>\n  \n  <script class=\"pd_save\">\n    $(function() {\n      var tableWrapper = $('.df-table-wrapper-d3e86100');\n      var fixedHeader = $('.fixed-header', tableWrapper);\n      var tableContainer = $('.df-table-container', tableWrapper);\n      var table = $('.df-table', tableContainer);\n      var rows = $('tbody > tr', table);\n      var total = 14;\n  \n      fixedHeader\n        .css('width', table.width())\n        .find('.fixed-cell')\n        .each(function(i, e) {\n          $(this).css('width', $('.df-table-wrapper-d3e86100 th:nth-child(' + (i+1) + ')').css('width'));\n        });\n  \n      tableContainer.scroll(function() {\n        fixedHeader.css({ left: table.position().left });\n      });\n  \n      rows.on(\"click\", function(e){\n          var txt = e.delegateTarget.innerText;\n          var splits = txt.split(\"\\t\");\n          var len = splits.length;\n          var hdrs = $(fixedHeader).find(\".fixed-cell\");\n          // Add all cells in the selected row as a map to be consumed by the target as needed\n          var payload = {type:\"select\", targetDivId: \"\" };\n          for (var i = 0; i < len; i++) {\n            payload[hdrs[i].innerHTML] = splits[i];\n          }\n  \n          //simple selection highlighting, client adds \"selected\" class\n          $(this).addClass(\"selected\").siblings().removeClass(\"selected\");\n          $(document).trigger('pd_event', payload);\n      });\n  \n      $('.df-table-search', tableWrapper).keyup(function() {\n        var val = '^(?=.*\\\\b' + $.trim($(this).val()).split(/\\s+/).join('\\\\b)(?=.*\\\\b') + ').*$';\n        var reg = RegExp(val, 'i');\n        var index = 0;\n        \n        rows.each(function(i, e) {\n          if (!reg.test($(this).text().replace(/\\s+/g, ' '))) {\n            $(this).attr('class', 'hidden');\n          }\n          else {\n            $(this).attr('class', (++index % 2 == 0 ? 'even' : 'odd'));\n          }\n        });\n        $('.df-table-search-count', tableWrapper).html('Showing ' + index + ' of ' + total + ' rows');\n      });\n    });\n  \n    $(\".df-table-wrapper td:contains('http://')\").each(function(){var tc = this.textContent; $(this).wrapInner(\"<a target='_blank' href='\" + tc + \"'></a>\");});\n    $(\".df-table-wrapper td:contains('https://')\").each(function(){var tc = this.textContent; $(this).wrapInner(\"<a target='_blank' href='\" + tc + \"'></a>\");});\n  </script>\n  \n        </div>",
                        "text/plain": "<IPython.core.display.HTML object>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": "import pixiedust\nfrom pyspark.sql.functions import col\nc = df.groupBy('class').count().orderBy('count')\ndisplay(c, sorted=true)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Imbalanced classes can cause pain in machine learning. Therefore let\u2019s rebalance. In the flowing we limit the number of elements per class to the amount of the least represented class. This is called undersampling. Other ways of rebalancing can be found here:\n\nhttps://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.sql.functions import min\n\n# create a lot of distinct classes from the dataset\nclasses = [row[0] for row in df.select('class').distinct().collect()]\n\n# compute the number of elements of the smallest class in order to limit the number of samples per calss\nmin = df.groupBy('class').count().select(min('count')).first()[0]\n\n# define the result dataframe variable\ndf_balanced = None\n\n#\u00a0iterate over distinct classes\nfor cls in classes:\n    \n    #\u00a0only select examples for the specific class within this iteration\n    # shuffle the order of the elements (by setting fraction to 1.0 sample works like shuffle)\n    # return only the first n samples\n    df_temp = df \\\n        .filter(\"class = '\"+cls+\"'\") \\\n        .sample(False, 1.0) \\\n        .limit(min)\n    \n    # on first iteration, assing df_temp to empty df_balanced\n    if df_balanced == None:    \n        df_balanced = df_temp\n    # afterwards, append vertically\n    else:\n        df_balanced=df_balanced.union(df_temp)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Please verify, by using the code cell below, if df_balanced has the same number of elements per class. You should get 6683 elements per class."
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n| Use_telephone| 6683|\n| Standup_chair| 6683|\n|      Eat_meat| 6683|\n|     Getup_bed| 6683|\n|   Drink_glass| 6683|\n|    Pour_water| 6683|\n|     Comb_hair| 6683|\n|          Walk| 6683|\n|  Climb_stairs| 6683|\n| Sitdown_chair| 6683|\n|   Liedown_bed| 6683|\n|Descend_stairs| 6683|\n|   Brush_teeth| 6683|\n|      Eat_soup| 6683|\n+--------------+-----+\n\n"
                }
            ],
            "source": "df_balanced.groupBy('class').count().show()"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "df=sc.parallelize([1,2,4,5,34,1,32,4,34,2,1,3])"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "12"
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df.count()"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "10.25"
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "tt=df.sum()\nn=df.count()\nmean=float(tt/n)\nmean"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_new=df.sortBy(lamda x : x)\nif(n % 2 ==1):\n    "
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}